# estimate the pose of target objects detected
import numpy as np
import json
import os
import ast
import cv2
from YOLO.detector import Detector
from sklearn.cluster import DBSCAN


# list of target fruits and vegs types
# Make sure the names are the same as the ones used in your YOLO model
TARGET_TYPES = ['pear', 'lemon', 'lime', 'tomato', 'capsicum', 'potato', 'pumpkin', 'garlic']


def estimate_pose(camera_matrix, obj_info, robot_pose):
    """
    function:
        estimate the pose of a target based on size and location of its bounding box and the corresponding robot pose
    input:
        camera_matrix: list, the intrinsic matrix computed from camera calibration (read from 'param/intrinsic.txt')
            |f_x, s,   c_x|
            |0,   f_y, c_y|
            |0,   0,   1  |
            (f_x, f_y): focal length in pixels
            (c_x, c_y): optical centre in pixels
            s: skew coefficient (should be 0 for PenguinPi)
        obj_info: list, an individual bounding box in an image (generated by get_bounding_box, [label,[x,y,width,height]])
        robot_pose: list, pose of robot corresponding to the image (read from 'lab_output/images.txt', [x,y,theta])
    output:
        target_pose: dict, prediction of target pose
    """
    # read in camera matrix (from camera calibration results)
    focal_length = camera_matrix[0][0]

    # there are 8 possible types of fruits and vegs
    ######### Replace with your codes #########
    # TODO: measure actual sizes of targets [width, depth, height] and update the dictionary of true target dimensions
    target_dimensions_dict = {'pear': [77/1000,71/1000,85/1000], 'lemon': [76/1000,51/1000,45/1000], 
                              'lime': [75/1000,52/1000,51/1000], 'tomato': [68/1000,70/1000,58/1000], 
                              'capsicum': [75/1000,70/1000,79/1000], 'potato': [96/1000,65/1000,60/1000], 
                              'pumpkin': [85/1000,83/1000,54/1000], 'garlic': [63/1000,61/1000,70/1000]}
    #########

    # estimate target pose using bounding box and robot pose
    target_class = obj_info[0]     # get predicted target label of the box
    target_box = obj_info[1]       # get bounding box measures: [x,y,width,height]
    true_height = target_dimensions_dict[target_class][2]   # look up true height of by class label
    true_width = target_dimensions_dict[target_class][1]    # look up true width by class 

    # compute pose of the target based on bounding box info, true object height, and robot's pose
    pixel_height = target_box[3]
    pixel_width = target_box[2]
    pixel_center = target_box[0]
    distance = true_height/pixel_height * focal_length  # estimated distance between the robot and the centre of the image plane based on height
    if target_class == 'tomato' or target_class == 'pumpkin':
        #print(target_class)
        distance_width = true_width/pixel_width * focal_length
        distance = (distance + distance_width) / 2
    # training image size 320x240p

    # remove estimates from far away, remove if statement if you want to keep all estimates
    
    image_width = 320 # change this if your training image is in a different size (check details of pred_0.png taken by your robot)
    x_shift = image_width/2 - pixel_center              # x distance between bounding box centre and centreline in camera view
    theta = np.arctan(x_shift/focal_length)     # angle of object relative to the robot
    ang = theta + robot_pose[2]     # angle of object in the world frame
    
   # relative object location
    distance_obj = distance/np.cos(theta) # relative distance between robot and object
    #distance_obj = 1.052282275*distance_obj + 0.000364838

    #print(distance_obj)
    x_relative = distance_obj * np.cos(theta) # relative x pose
    y_relative = distance_obj * np.sin(theta) # relative y pose
    relative_pose = {'x': x_relative, 'y': y_relative}
    #print(f'relative_pose: {relative_pose}')

    # location of object in the world frame using rotation matrix
    delta_x_world = x_relative * np.cos(robot_pose[2]) - y_relative * np.sin(robot_pose[2])
    delta_y_world = x_relative * np.sin(robot_pose[2]) + y_relative * np.cos(robot_pose[2])
    # add robot pose with delta target pose
    y = (robot_pose[1]+delta_y_world)[0]
    x = (robot_pose[0]+delta_x_world)[0]

    # if target_class == 'tomato':
    #     x = 0.995270821*x + 0.008616314
    # elif target_class == 'garlic':
    #     x = 1.137626098*x + 0.004955899
    # elif target_class == 'pumpkin':
    #     x = 1.052282275*x + 0.000364838
    # elif target_class == 'capsicum':
    #     x = 1.024165395*x - 0.000642935
    # elif target_class == 'pear':
    #     x = 0.962332343*x + 0.010300807
    # elif target_class == 'potato':
    #     x = 1.035078309*x - 0.015189712
    # elif target_class == 'lemon':
    #     x = 1.064043241*x - 0.014865542
    # elif target_class == 'lime':
    #     x = 1.033869384*x - 0.026315462
    target_pose = {'y':y ,
                      'x': x}
    #print(f'delta_x_world: {delta_x_world}, delta_y_world: {delta_y_world}')
    #print(f'target_pose: {target_pose}')

    return target_pose


def merge_estimations(target_pose_dict):
    """
    function:
        merge estimations of the same target
    input:
        target_pose_dict: dict, generated by estimate_pose
    output:
        target_est: dict, target pose estimations after merging
    """

    ######### Replace with your codes #########
    # TODO: replace it with a solution to merge the multiple occurrences of the same class type (e.g., by a distance threshold)
    #target_est = target_pose_dict
    #########
    target_est = {}
    distance_threshold = 0.3
    fruits = ['pear', 'lemon', 'lime', 'tomato', 'capsicum', 'pumpkin', 'garlic', 'potato']
     # Filter out poses outside the valid area
    valid_pose_dict = {key: pose for key, pose in target_pose_dict.items() if -1.55 < pose['x'] < 1.55 and -1.55 < pose['y'] < 1.55}


    for key, pose in valid_pose_dict.items():
        target_type = key.split('_')[0]

        if target_type not in target_est:
            target_est[target_type] = []

        target_est[target_type].append(pose)

    # Debug: Check if poses are being grouped
    #print(f"Grouped Target Poses (before merging): {target_est}")

    final_target_est = {}
    for target_type, poses in target_est.items():
        if len(poses) == 1:
            final_target_est[f"{target_type}_1"] = poses[0]
            continue

        # Convert list of dicts to numpy array
        poses_array = np.array([[pose['x'], pose['y']] for pose in poses])

        # Use DBSCAN to cluster poses based on distance
        clustering = DBSCAN(eps=distance_threshold, min_samples=1).fit(poses_array)
        labels = clustering.labels_

        # Merge poses within each cluster
        for cluster_id in set(labels):
            cluster_poses = poses_array[labels == cluster_id]
                    # Filter out clusters with less than 5 pose estimations
            if len(cluster_poses) < 5:
                continue
            centroid = np.mean(cluster_poses, axis=0)
            final_target_est[f"{target_type}_{cluster_id}"] = {'x': centroid[0], 'y': centroid[1]}

    #finding missing fruits        
    missing_objects = [obj for obj in fruits if obj not in final_target_est]

    return final_target_est, missing_objects

def reprocess_missing_objects(missing_objects, target_pose_dict, distance_threshold=0.3):
    """
    Reprocess and merge estimations for missing objects.
    """
    additional_target_est = {}

    for missing_object in missing_objects:
        # Extract poses for the missing object
        
        missing_object_poses = {key: pose for key, pose in target_pose_dict.items() if key.startswith(missing_object)}

        if not missing_object_poses:
            continue

        # Convert poses to array for clustering
        poses_array = np.array([[pose['x'], pose['y']] for pose in missing_object_poses.values()])

        # Use DBSCAN to cluster poses based on distance
        clustering = DBSCAN(eps=distance_threshold, min_samples=1).fit(poses_array)
        labels = clustering.labels_

        # Merge poses within each cluster
        for cluster_id in set(labels):
            cluster_poses = poses_array[labels == cluster_id]
            centroid = np.mean(cluster_poses, axis=0)
            additional_target_est[f"{missing_object}_{cluster_id}"] = {'x': centroid[0], 'y': centroid[1]}

    return additional_target_est
   
# main loop
if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.abspath(__file__))     # get current script directory (TargetPoseEst.py)

    # read in camera matrix
    fileK = f'{script_dir}/calibration/param/intrinsic.txt'
    camera_matrix = np.loadtxt(fileK, delimiter=',')

    # init YOLO model
    model_path = f'{script_dir}/YOLO/model/yolov8_model.pt'
    yolo = Detector(model_path)

    # create a dictionary of all the saved images with their corresponding robot pose
    image_poses = {}
    with open(f'{script_dir}/lab_output/images.txt') as fp:
        for line in fp.readlines():
            pose_dict = ast.literal_eval(line)
            image_poses[pose_dict['imgfname']] = pose_dict['pose']

    # estimate pose of targets in each image
    target_pose_dict = {}
    detected_type_list = []
    for image_path in image_poses.keys():
        input_image = cv2.imread(image_path)
        bounding_boxes, bbox_img = yolo.detect_single_image(input_image)
        # cv2.imshow('bbox', bbox_img)
        # cv2.waitKey(0)
        robot_pose = image_poses[image_path]

        for detection in bounding_boxes:
            # count the occurrence of each target type
            occurrence = detected_type_list.count(detection[0])
            target_pose_dict[f'{detection[0]}_{occurrence}'] = estimate_pose(camera_matrix, detection, robot_pose)

            detected_type_list.append(detection[0])

    # merge the estimations of the targets so that there are at most 3 estimations of each target type
    target_est = {}
    target_est, missing_objects = merge_estimations(target_pose_dict)
    #if missing_objects:
        #print(f"Missing objects: {missing_objects}")
        #additional_target_est = reprocess_missing_objects(missing_objects, target_pose_dict)
        #target_est.update(additional_target_est)
    print(target_est)

    # save target pose estimations
    with open(f'{script_dir}/lab_output/targets.txt', 'w') as fo:
        json.dump(target_est, fo, indent=4)

    print('Estimations saved!')
