# estimate the pose of target objects detected
import numpy as np
import json
import os
import ast
import cv2
from detector import Detector
from util.clear_out import clear

# list of target fruits and vegs types
# Make sure the names are the same as the ones used in your YOLO model
TARGET_TYPES = ['pear', 'lemon', 'lime', 'tomato', 'capsicum', 'potato', 'pumpkin', 'garlic']


def estimate_pose(camera_matrix, obj_info, robot_pose):
    """e
    function:
        estimate the pose of a target based on size and location of its bounding box and the corresponding robot pose
    input:
        camera_matrix: list, the intrinsic matrix computed from camera calibration (read from 'param/intrinsic.txt')
            |f_x, s,   c_x|
            |0,   f_y, c_y|
            |0,   0,   1  |
            (f_x, f_y): focal length in pixels
            (c_x, c_y): optical centre in pixels
            s: skew coefficient (should be 0 for PenguinPi)
        obj_info: list, an individual bounding box in an image (generated by get_bounding_box, [label,[x,y,width,height]])
        robot_pose: list, pose of robot corresponding to the image (read from 'lab_output/images.txt', [x,y,theta])
    output:
        target_pose: dict, prediction of target pose
    """
    # read in camera matrix (from camera calibration results)
    focal_length = camera_matrix[0][0]

    # there are 8 possible types of fruits and vegs
    ######### Replace with your codes #########
    # TODO: measure actual sizes of targets [width, depth, height] and update the dictionary of true target dimensions
    target_dimensions_dict = {'pear': [0.07,0.065,0.105], 'lemon': [0.075, 0.045, 0.06], 
                              'lime': [0.07, 0.045, 0.045], 'tomato': [0.065, 0.065, 0.063], 
                              'capsicum': [0.07, 0.07, 0.094], 'potato': [0.09, 0.06, 0.05], 
                              'pumpkin': [0.085, 0.085 ,0.085], 'garlic': [0.063, 0.065, 0.075]}
    #########

    # estimate target pose using bounding box and robot pose
    target_class = obj_info[0]     # get predicted target label of the box

    target_box = obj_info[2]       # get bounding box measures: [x,y,width,height]
    true_height = target_dimensions_dict[target_class][2]   # look up true height of by class label

    true_width = (target_dimensions_dict[target_class][0] + target_dimensions_dict[target_class][1]) / 2
    true_propotion = true_width / true_height

    # compute pose of the target based on bounding box info, true object height, and robot's pose
    pixel_height = target_box[3]
    pixel_center_x = target_box[0]

    pixel_width = target_box[2]
    pixel_center_y = target_box[1]

    bbox_proportion = pixel_width/pixel_height
    error = abs(true_propotion - bbox_proportion) 
    #print(target_class, " bbox error: ", error)

    if target_class == "capsicum":
         if error > 0.25:
            # Fruit is cut off, it will give bad reading, therefore dont detect
            return None, None
    elif target_class == "potato":
         if error > 0.5:
            # Fruit is cut off, it will give bad reading, therefore dont detect
            return None, None
    elif (target_class == "lime") or (target_class == "lemon"):
         if error > 0.4:
            # Fruit is cut off, it will give bad reading, therefore dont detect
            return None, None
    else:
        if error > 0.2:
            # Fruit is cut off, it will give bad reading, therefore dont detect
            return None, None


    distance = true_height/pixel_height * focal_length  # estimated distance between the robot and the centre of the image plane based on height
    
    # Add 2 cm to centre of fruit (can change this value?)
    distance += 0.03

    # training image size 320x240p
    image_width = 320 # change this if your training image is in a different size (check details of pred_0.png taken by your robot)
    x_shift = image_width/2 - pixel_center_x              # x distance between bounding box centre and centreline in camera view
    theta = np.arctan(x_shift/focal_length)     # angle of object relative to the robot
    ang = theta + robot_pose[2]     # angle of object in the world frame
    
   # relative object location
    distance_obj = distance/np.cos(theta) # relative distance between robot and object
    x_relative = distance_obj * np.cos(theta) # relative x pose
    y_relative = distance_obj * np.sin(theta) # relative y pose
    relative_pose = {'x': x_relative, 'y': y_relative}
    #print(f'relative_pose: {relative_pose}')

    # location of object in the world frame using rotation matrix
    delta_x_world = x_relative * np.cos(robot_pose[2]) - y_relative * np.sin(robot_pose[2])
    delta_y_world = x_relative * np.sin(robot_pose[2]) + y_relative * np.cos(robot_pose[2])
    # add robot pose with delta target pose
    target_pose = {'y': (robot_pose[1]+delta_y_world)[0],
                   'x': (robot_pose[0]+delta_x_world)[0],
                   'known': False}
    #print(f'delta_x_world: {delta_x_world}, delta_y_world: {delta_y_world}')
    #print(f'target_pose: {target_pose}')

    return target_pose, relative_pose


def merge_estimations(target_pose_dict, wp_distance_threshold  = 0.1):
    target_est = {}
    distance_threshold = 0.6  # Adjust this threshold based on your requirement
    update_wp_distance_threshold = wp_distance_threshold
    update_wp_distance = False
    merged_total = False
    
    for key, pose in target_pose_dict.items():
        
        target_type = key.split('_')[0]
        if target_type not in target_est:
            target_est[target_type] = []

        merged = False
        for existing_pose in target_est[target_type]:
            dist = np.sqrt((existing_pose['x'] - pose['x'])**2 + (existing_pose['y'] - pose['y'])**2)
            if dist < distance_threshold:
                #existing_pose['x'] = (existing_pose['x'] + pose['x']) / 2
                #existing_pose['y'] = (existing_pose['y'] + pose['y']) / 2
                existing_pose['x'] = (existing_pose['x']*1.8 + pose['x']*0.2) / 2
                existing_pose['y'] = (existing_pose['y']*1.8 + pose['y']*0.2) / 2
                merged = True
                if dist > update_wp_distance_threshold:
                    #print("FRUIT MOVED : ", dist)
                    update_wp_distance = True

        if not merged:
            target_est[target_type].append(pose)
        else:
            merged_total = True


                
    # Flatten the result to have at most 3 estimations per target type
    final_est = {}
    for target_type, poses in target_est.items():
        for i, pose in enumerate(poses[:3]):  # Limit to at most 3 estimations per target type
            final_est[f'{target_type}_{i}'] = pose
    return final_est, update_wp_distance, merged_total
