# estimate the pose of target objects detected
import numpy as np
import json
import os
import ast
import cv2
from YOLO.detector import Detector
from itertools import combinations
import math
from sklearn.cluster import KMeans

classLabels = {'0': 'tomato',
                       '1': 'capsicum',
                        '2': 'garlic',
                        '3': 'lemon',
                        '4': 'lime',
                        '5': 'pear',
                        '6': 'potato',
                        '7': 'pumpkin',
                        'tomato': 'tomato',
                        'capsicum': 'capsicum',
                        'garlic': 'garlic',
                        'lemon': 'lemon',
                        'lime': 'lime',
                        'pear': 'pear',
                        'potato': 'potato',
                        'pumpkin': 'pumpkin'}
target_dimensions_dict = {'pear': [77/1000,71/1000,85/1000], 'lemon': [0.074,0.05,0.052], 
                              'lime': [0.07,0.05,0.051], 'tomato': [0.07,0.06,0.06], 
                              'capsicum': [0.073,0.073,0.09], 'potato': [0.09,0.069,0.058], 
                              'pumpkin': [0.08,0.08,0.07], 'garlic': [0.065,0.06,0.076]}    #########

def estimate_pose(camera_matrix, obj_info, robot_pose):
    """
    function:
        estimate the pose of a target based on size and location of its bounding box and the corresponding robot pose
    input:
        camera_matrix: list, the intrinsic matrix computed from camera calibration (read from 'param/intrinsic.txt')
            |f_x, s,   c_x|
            |0,   f_y, c_y|
            |0,   0,   1  |
            (f_x, f_y): focal length in pixels
            (c_x, c_y): optical centre in pixels
            s: skew coefficient (should be 0 for PenguinPi)
        obj_info: list, an individual bounding box in an image (generated by get_bounding_box, [label,[x,y,width,height]])
        robot_pose: list, pose of robot corresponding to the image (read from 'lab_output/images.txt', [x,y,theta])
    output:
        target_pose: dict, prediction of target pose
    """
    # read in camera matrix (from camera calibration results)
    focal_length = camera_matrix[0][0]

    # there are 8 possible types of fruits and vegs
    ######### Replace with your codes #########
    # TODO: measure actual sizes of targets [width, depth, height] and update the dictionary of true target dimensions
    target_dimensions_dict = {'pear': [77/1000,71/1000,85/1000], 'lemon': [0.074,0.05,0.06], 
                              'lime': [0.07,0.05,0.056], 'tomato': [0.07,0.065,0.069], 
                              'capsicum': [0.073,0.073,0.085], 'potato': [0.09,0.069,0.07], 
                              'pumpkin': [0.08,0.08,0.07], 'garlic': [0.065,0.06,0.08]}
    #########

    # estimate target pose using bounding box and robot pose
    target_class = obj_info[0]     # get predicted target label of the box
    target_box = obj_info[1]       # get bounding box measures: [x,y,width,height]
    true_height = target_dimensions_dict[target_class][2]   # look up true height of by class label
    true_width = target_dimensions_dict[target_class][1]    # look up true width by class 

    # compute pose of the target based on bounding box info, true object height, and robot's pose
    pixel_height = target_box[3]
    pixel_width = target_box[2]
    pixel_center = target_box[0]
    distance = true_height/pixel_height * focal_length  # estimated distance between the robot and the centre of the image plane based on height
    if  target_class == 'pumpkin': #or target_class == 'capsicum':
        #print(target_class)
        distance_width = true_width/pixel_width * focal_length
        distance = (distance + distance_width) / 2
    # training image size 320x240p
    image_width = 320 # change this if your training image is in a different size (check details of pred_0.png taken by your robot)
    x_shift = image_width/2 - pixel_center              # x distance between bounding box centre and centreline in camera view
    theta = np.arctan(x_shift/focal_length)     # angle of object relative to the robot
    ang = theta + robot_pose[2]     # angle of object in the world frame
    
   # relative object location
    distance_obj = distance/np.cos(theta) # relative distance between robot and object
    x_relative = distance_obj * np.cos(theta) # relative x pose
    y_relative = distance_obj * np.sin(theta) # relative y pose
    relative_pose = {'x': x_relative, 'y': y_relative}
    #print(f'relative_pose: {relative_pose}')

    # location of object in the world frame using rotation matrix
    delta_x_world = x_relative * np.cos(robot_pose[2]) - y_relative * np.sin(robot_pose[2])
    delta_y_world = x_relative * np.sin(robot_pose[2]) + y_relative * np.cos(robot_pose[2])
    y = (robot_pose[1]+delta_y_world)[0]
    x = (robot_pose[0]+delta_x_world)[0]


    # add robot pose with delta target pose
    target_pose = {'y': y,
                   'x':x}
    #print(f'delta_x_world: {delta_x_world}, delta_y_world: {delta_y_world}')
    #print(f'target_pose: {target_pose}')

    return target_pose

def get_distance(head,toe):
    ## Gets euclidian distance between two coordinates from dict {'y': 0.123,'x',0.456}
    if isinstance(head,dict):
        yDif = head['y']-toe['y']
        xDif = head['x']-toe['x']
    else:
        xDif = head[0]-toe[0]
        yDif = head[1]-toe[1]
    return math.sqrt(yDif**2 + xDif**2)

def merge_estimations(target_pose_dict):
    """
    function:
        merge estimations of the same target using K-means clustering
    input:
        target_pose_dict: dict, generated by estimate_pose
    output:
        target_est: dict, target pose estimations after merging
    """
    """
    function:
        merge estimations of the same target
    input:
        target_pose_dict: dict, generated by estimate_pose
    output:
        target_est: dict, target pose estimations after merging
    """
    target_est = {}
    detected_type_list = []
    target_dict = {'pear':[],
                    'lemon':[],
                    'lime':[],
                    'tomato':[],
                    'capsicum':[],
                    'potato':[],
                    'pumpkin':[],
                    'garlic':[]}
    n_clusters = 2

    ######### Replace with your codes #########
    # TODO: replace it with a solution to merge the multiple occurrences of the same class type (e.g., by a distance threshold)
    # sorts all detection into dictionary of lists keyed by target type
    for target in target_pose_dict.keys():
        target_dict[classLabels[target.split('_')[0]]].append(target_pose_dict[target])

    for fruit,detection_list in target_dict.items():
        if not detection_list: # list is empty
            continue
        #print(fruit)
        if len(detection_list) < n_clusters:
            x = 0
            y = 0
            for detection in detection_list:
                x += detection['x']
                y += detection['y']
            x /= len(detection_list)
            y /= len(detection_list)
            target_est[f'{fruit}_{detected_type_list.count(fruit)}'] = {'y':y,'x':x}
            detected_type_list.append(fruit)
        else:
            dataPoints = []
            for detection in detection_list:
                dataPoints.append([detection['x'],detection['y']])

            kmeans = KMeans(n_clusters=n_clusters, random_state=0,n_init='auto').fit(dataPoints)

            cluster_centers = []

            for estCenter in kmeans.cluster_centers_:
                cluster_centers.append([estCenter[0],estCenter[1]])
                
            #print(list(combinations(cluster_centers,2)))
            while True:
                for combination in list(combinations(cluster_centers,2)):
                    #print("cur combo:\n",combination)
                    if get_distance(combination[0],combination[1]) < 0.5:
                        newX = (combination[0][0]+combination[1][0])/2
                        newY = (combination[0][1]+combination[1][1])/2
                        cluster_centers.remove(combination[0])
                        cluster_centers.remove(combination[1])
                        cluster_centers.append([newX,newY])
                        #print("merged")
                        #print([newX,newY])
                        break
                break
            for grouped_centers in cluster_centers:
                target_est[f'{fruit}_{detected_type_list.count(fruit)}'] = {'y':grouped_centers[1],'x':grouped_centers[0]}
                detected_type_list.append(fruit)
        
    #######
    return target_est


if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.abspath(__file__))     # get current script directory (TargetPoseEst.py)

    # read in camera matrix
    fileK = f'{script_dir}/calibration/param/intrinsic.txt'
    camera_matrix = np.loadtxt(fileK, delimiter=',')

    # init YOLO model
    model_path = f'{script_dir}/YOLO/model/yolov8_model.pt'
    yolo = Detector(model_path)

    # create a dictionary of all the saved images with their corresponding robot pose
    image_poses = {}
    with open(f'{script_dir}/lab_output/images.txt') as fp:
        for line in fp.readlines():
            pose_dict = ast.literal_eval(line)
            image_poses[pose_dict['imgfname']] = pose_dict['pose']

    # estimate pose of targets in each image
    target_pose_dict = {}
    detected_type_list = []
    for image_path in image_poses.keys():
        input_image = cv2.imread(image_path)
        bounding_boxes, bbox_img = yolo.detect_single_image(input_image)
        # cv2.imshow('bbox', bbox_img)
        # cv2.waitKey(0)
        robot_pose = image_poses[image_path]

        for detection in bounding_boxes:
            # count the occurrence of each target type
            occurrence = detected_type_list.count(detection[0])
            target_pose_dict[f'{detection[0]}_{occurrence}'] = estimate_pose(camera_matrix, detection, robot_pose)

            detected_type_list.append(detection[0])

    # merge the estimations of the targets so that there are at most 3 estimations of each target type
    target_est = {}
    target_est = merge_estimations(target_pose_dict)
        # Filter out poses that fall outside the range -1.8 < x < 1.8 and -1.8 < y < 1.8
    #filtered_target_est = {k: v for k, v in target_est.items() if -1.8 < v['x'] < 1.8 and -1.8 < v['y'] < 1.8}
    #print(target_est)
    # save target pose estimations
    with open(f'{script_dir}/lab_output/targets.txt', 'w') as fo:
        json.dump(target_est, fo, indent=4)

    print('Estimations saved!')