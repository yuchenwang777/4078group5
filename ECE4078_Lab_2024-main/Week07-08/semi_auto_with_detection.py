# M4 - Autonomous fruit searching

# basic python packages
import sys, os
import cv2
import numpy as np
import json
import argparse
import time
import tkinter as tk
from PIL import Image, ImageTk, ImageDraw
import math
import argparse
from obstacles import *
from operate import Operate
from sklearn.cluster import DBSCAN


# import SLAM components
sys.path.insert(0, "{}/slam".format(os.getcwd()))
from slam.ekf import EKF
from slam.robot import Robot
import slam.aruco_detector as aruco
from YOLO.detector import Detector

# import utility functions
sys.path.insert(0, "util")
from pibot import PenguinPi
import measure as measure

import numpy as np
import random
import math
import matplotlib.pyplot as plt

def estimate_pose(camera_matrix, obj_info, robot_pose):
    """
    function:
        estimate the pose of a target based on size and location of its bounding box and the corresponding robot pose
    input:
        camera_matrix: list, the intrinsic matrix computed from camera calibration (read from 'param/intrinsic.txt')
            |f_x, s,   c_x|
            |0,   f_y, c_y|
            |0,   0,   1  |
            (f_x, f_y): focal length in pixels
            (c_x, c_y): optical centre in pixels
            s: skew coefficient (should be 0 for PenguinPi)
        obj_info: list, an individual bounding box in an image (generated by get_bounding_box, [label,[x,y,width,height]])
        robot_pose: list, pose of robot corresponding to the image (read from 'lab_output/images.txt', [x,y,theta])
    output:
        target_pose: dict, prediction of target pose
    """
    # read in camera matrix (from camera calibration results)
    focal_length = camera_matrix[0][0]

    # there are 8 possible types of fruits and vegs
    ######### Replace with your codes #########
    # TODO: measure actual sizes of targets [width, depth, height] and update the dictionary of true target dimensions
    target_dimensions_dict = {'pear': [77/1000,71/1000,106/1000], 'lemon': [76/1000,51/1000,50/1000], 
                              'lime': [75/1000,52/1000,51/1000], 'tomato': [68/1000,70/1000,58/1000], 
                              'capsicum': [75/1000,70/1000,79/1000], 'potato': [96/1000,65/1000,60/1000], 
                              'pumpkin': [85/1000,83/1000,54/1000], 'garlic': [63/1000,61/1000,70/1000]}
    #########

    # estimate target pose using bounding box and robot pose
    target_class = obj_info[0]     # get predicted target label of the box
    target_box = obj_info[1]       # get bounding box measures: [x,y,width,height]
    true_height = target_dimensions_dict[target_class][2]   # look up true height of by class label

    # compute pose of the target based on bounding box info, true object height, and robot's pose
    pixel_height = target_box[3]
    pixel_center = target_box[0]
    distance = true_height/pixel_height * focal_length  # estimated distance between the robot and the centre of the image plane based on height
    # training image size 320x240p
    image_width = 320 # change this if your training image is in a different size (check details of pred_0.png taken by your robot)
    x_shift = image_width/2 - pixel_center              # x distance between bounding box centre and centreline in camera view
    theta = np.arctan(x_shift/focal_length)     # angle of object relative to the robot
    ang = theta + robot_pose[2]     # angle of object in the world frame
    
   # relative object location
    distance_obj = distance/np.cos(theta) # relative distance between robot and object
    x_relative = distance_obj * np.cos(theta) # relative x pose
    y_relative = distance_obj * np.sin(theta) # relative y pose
    relative_pose = {'x': x_relative, 'y': y_relative}
    #print(f'relative_pose: {relative_pose}')

    # location of object in the world frame using rotation matrix
    delta_x_world = x_relative * np.cos(robot_pose[2]) - y_relative * np.sin(robot_pose[2])
    delta_y_world = x_relative * np.sin(robot_pose[2]) + y_relative * np.cos(robot_pose[2])
    # add robot pose with delta target pose
    target_pose = {'y': (robot_pose[1]+delta_y_world)[0],
                   'x': (robot_pose[0]+delta_x_world)[0]}
    #print(f'delta_x_world: {delta_x_world}, delta_y_world: {delta_y_world}')
    #print(f'target_pose: {target_pose}')

    return target_pose

def merge_estimations(target_pose_dict):
    """
    function:
        merge estimations of the same target
    input:
        target_pose_dict: dict, generated by estimate_pose
    output:
        target_est: dict, target pose estimations after merging
    """

    ######### Replace with your codes #########
    # TODO: replace it with a solution to merge the multiple occurrences of the same class type (e.g., by a distance threshold)
    #target_est = target_pose_dict
    #########
    target_est = {}
    distance_threshold = 0.3
     # Filter out poses outside the valid area
    valid_pose_dict = {key: pose for key, pose in target_pose_dict.items() if -1.5 < pose['x'] < 1.5 and -1.5 < pose['y'] < 1.5}


    for key, pose in valid_pose_dict.items():
        target_type = key.split('_')[0]

        if target_type not in target_est:
            target_est[target_type] = []

        target_est[target_type].append(pose)

    # Debug: Check if poses are being grouped
    print(f"Grouped Target Poses (before merging): {target_est}")

    final_target_est = {}
    for target_type, poses in target_est.items():
        if len(poses) == 1:
            final_target_est[f"{target_type}_1"] = poses[0]
            continue

        # Convert list of dicts to numpy array
        poses_array = np.array([[pose['x'], pose['y']] for pose in poses])

        # Use DBSCAN to cluster poses based on distance
        clustering = DBSCAN(eps=distance_threshold, min_samples=1).fit(poses_array)
        labels = clustering.labels_

        # Merge poses within each cluster
        for cluster_id in set(labels):
            cluster_poses = poses_array[labels == cluster_id]
                    # Filter out clusters with less than 5 pose estimations
            centroid = np.mean(cluster_poses, axis=0)
            final_target_est[f"{target_type}_{cluster_id}"] = {'x': centroid[0], 'y': centroid[1]}

    return final_target_est


def read_true_map(fname):
    """Read the ground truth map and output the pose of the ArUco markers and 5 target fruits&vegs to search for

    @param fname: filename of the map
    @return:
        1) list of targets, e.g. ['lemon', 'tomato', 'garlic']
        2) locations of the targets, [[x1, y1], ..... [xn, yn]]
        3) locations of ArUco markers in order, i.e. pos[9, :] = position of the aruco10_0 marker
    """
    with open(fname, 'r') as fd:
        gt_dict = json.load(fd)
        fruit_list = []
        fruit_true_pos = []
        aruco_true_pos = np.empty([10, 2])

        # remove unique id of targets of the same type
        for key in gt_dict:
            x = np.round(gt_dict[key]['x'], 1)
            y = np.round(gt_dict[key]['y'], 1)

            if key.startswith('aruco'):
                if key.startswith('aruco10'):
                    aruco_true_pos[9][0] = x
                    aruco_true_pos[9][1] = y
                else:
                    marker_id = int(key[5]) - 1
                    aruco_true_pos[marker_id][0] = x
                    aruco_true_pos[marker_id][1] = y
            else:
                fruit_list.append(key[:-2])
                if len(fruit_true_pos) == 0:
                    fruit_true_pos = np.array([[x, y]])
                else:
                    fruit_true_pos = np.append(fruit_true_pos, [[x, y]], axis=0)

        return fruit_list, fruit_true_pos, aruco_true_pos


def read_search_list():
    """Read the search order of the target fruits

    @return: search order of the target fruits
    """
    search_list = []
    with open('M4_prac_shopping_list.txt', 'r') as fd:
        fruits = fd.readlines()

        for fruit in fruits:
            search_list.append(fruit.strip())

    return search_list


def print_target_fruits_pos(search_list, fruit_list, fruit_true_pos):
    """Print out the target fruits' pos in the search order

    @param search_list: search order of the fruits
    @param fruit_list: list of target fruits
    @param fruit_true_pos: positions of the target fruits
    """

    print("Search order:")
    n_fruit = 1
    target_positions = []
    for fruit in search_list:
        for i in range(len(fruit_list)): # there are 5 targets amongst 10 objects
            if fruit == fruit_list[i]:
                print('{}) {} at [{}, {}]'.format(n_fruit,
                                                  fruit,
                                                  np.round(fruit_true_pos[i][0], 1),
                                                  np.round(fruit_true_pos[i][1], 1)))
                target_positions.append(-1*fruit_true_pos[i])
        n_fruit += 1

     # Draw circles around the target fruits on the map image
    return target_positions


# Waypoint navigation
# the robot automatically drives to a given [x,y] coordinate
# note that this function requires your camera and wheel calibration parameters from M2, and the "util" folder from M1
# fully automatic navigation:
# try developing a path-finding algorithm that produces the waypoints automatically
def rotate_to_point(waypoint, robot_pose,ekf,robot):
    # imports camera / wheel calibration parameters 
    fileS = "calibration/param/scale.txt"
    scale = np.loadtxt(fileS, delimiter=',')
    fileB = "calibration/param/baseline.txt"
    baseline = np.loadtxt(fileB, delimiter=',')
    
    ####################################################
    # Calculate the angle to turn
    xg, yg = waypoint
    x,y,th = robot_pose
    
    
    # Normalize the angle to be within the range [-pi, pi]
    desired_angle = np.arctan2(yg - y, xg - x)
    current_angle = th
    angle_difference = desired_angle - current_angle
    while angle_difference>np.pi:
        angle_difference-=np.pi*2
    while angle_difference<=-np.pi:
        angle_difference+=np.pi*2

    wheel_vel = 30  # tick
    Kp = 0.5 # Proportional gain/ may need to change for better performance, if this works at all

    while abs(angle_difference) > 0.0005:  # Continue rotating until the angle error is small about 3 degrees can decrease if needed
        # Calculate the time to turn
        turn_time = Kp*abs(baseline * angle_difference / (scale * wheel_vel)) 

        # Turn the robot in the correct direction
        if angle_difference < 0:
            lv, rv = ppi.set_velocity([0, -1], turning_tick=wheel_vel, time=turn_time)
        else:
            lv, rv = ppi.set_velocity([0, 1], turning_tick=wheel_vel, time=turn_time)

        # Update the robot's pose using SLAM
        current_pose = get_robot_pose(ekf, robot, lv, rv, turn_time)
        x, y, th = current_pose

        # Recalculate the angle difference
        desired_angle = np.arctan2(yg - y, xg - x)
        angle_difference = desired_angle - th

        while angle_difference > np.pi:
            angle_difference -= np.pi * 2
        while angle_difference <= -np.pi:
            angle_difference += np.pi * 2

    print(f"expected angle: {(180/math.pi)*desired_angle}\nActual angle: {(180/math.pi)*th}")
    return lv,rv, turn_time, current_pose
    
    

def drive_to_point(waypoint, robot_pose):
    # imports camera / wheel calibration parameters 
    fileS = "calibration/param/scale.txt"
    scale = np.loadtxt(fileS, delimiter=',')
    fileB = "calibration/param/baseline.txt"
    baseline = np.loadtxt(fileB, delimiter=',')
    #(waypoint)
    #print(robot_pose[0], robot_pose[1],(180/math.pi)*robot_pose[2])
    
    ####################################################
    # Calculate the angle to turn
    delta_x = waypoint[0] - robot_pose[0]
    delta_y = waypoint[1] - robot_pose[1]
    prev_x = robot_pose[0]
    prev_y = robot_pose[1]
    # Calculate the distance to the waypoint
    distance = np.sqrt(delta_x ** 2 + delta_y ** 2)
    wheel_vel = 50  # tick
    distance_error = distance

    # Calculate drive time
    try:
        drive_time = distance / (wheel_vel*scale)
        if np.isnan(drive_time) or drive_time <= 0:
            raise ValueError("Invalid drive time calculated.")
    except Exception as e:
        #print(f"Error calculating drive time: {e}")
        drive_time = 1  # Set a default drive time

    #print(f"Driving for {drive_time:.2f} seconds")
    lv, rv = ppi.set_velocity([1, 0], tick=wheel_vel, time=drive_time)
    #robot_pose[:2] = waypoint
    current_pose = get_robot_pose(ekf,robot,lv,rv, drive_time)

    print(f"Expected waypoint [{waypoint[0]}, {waypoint[1]}]\nActual waypoint [{current_pose[0]}, {current_pose[1]}]")
    #return robot_pose
    return lv,rv, drive_time, current_pose


def get_robot_pose(ekf,robot,lv,rv, dt):
    ####################################################
    # TODO: replace with your codes to estimate the pose of the robot
    # We STRONGLY RECOMMEND you to use your SLAM code from M2 here

    drive_meas = measure.Drive(lv, -rv, dt)
    img = ppi.get_image_physical()
    aruco_detector = aruco.aruco_detector(robot)
    measurement, _ = aruco_detector.detect_marker_positions(img)
    ekf.predict(drive_meas)
    ekf.update(measurement)

    # update the robot pose [x,y,theta]
    state = ekf.get_state_vector() # replace with your calculation
    print(f"State: {state}")
    robot_pose = [state[0].item(), state[1].item(), state[2].item()]
    #print(f"Actual pose: {robot_pose}")
    ####################################################

    return robot_pose

def on_space(event):
    print("Spacebar pressed. Closing GUI.")
    root.quit()

def detect_obstacles(robot_pose, unknown_obstacles, target_pose_dict, map_image):
    # Get the image from the camera
    img = ppi.get_image_physical()
    # Detect the obstacles in the image
    bboxes, img_marked = yolo.detect_single_image(img)
    detected = False
    detected_location = None

    if bboxes:
        for bbox in bboxes:
            if bbox[0] not in search_list:
                target_pose = estimate_pose(camera_matrix, bbox, robot_pose)
                target_pose_dict[f"{bbox[0]}_{len(target_pose_dict)}"] = target_pose
                detected = True
                detected_location = (target_pose['x'], target_pose['y'])

                # Print the detected object's pose
                print(f"Detected {bbox[0]} at pose: {target_pose}")

                # Update the map image with the detected object
                draw = ImageDraw.Draw(map_image)
                x = int((1.5 - target_pose['x']) / 3 * map_image.width)
                y = int((target_pose['y'] + 1.5) / 3 * map_image.height)
                radius = 10  # Radius of the detected object
                draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill='blue')

    if detected:
        merged_obstacles = merge_estimations(target_pose_dict)
        new_unknown_obstacles = []
        for key, pose in merged_obstacles.items():
            new_unknown_obstacles.append(Circle(pose['x'], pose['y'], 0.15))

        # Update the unknown obstacles list by replacing old obstacles with new merged obstacles
        unknown_obstacles.clear()
        unknown_obstacles.extend(new_unknown_obstacles)

        cv2.imshow('Detected Obstacles', img_marked)
        cv2.waitKey(1)

    return unknown_obstacles, detected, detected_location, map_image

def on_click(event):
    global robot_pose, map_image_copy
    # Get the dimensions of the image
    img_width, img_height = map_image.size
    
    # Convert pixel coordinates to the desired range
    x = (event.x / img_width) * 3 - 1.5
    y = (event.y / img_height) * 3 - 1.5
    
    # based on the direction of your x/y axis my need to adjust x and y (+/-) to get the correct orientation
    # Invert x-axis to match typical coordinate system
    x = -x
    waypoint = [x,y]
    lv,rv,dt,robot_pose = rotate_to_point(waypoint, robot_pose,ekf,robot)
    unknown_obstacles, detected, detected_location, updated_map_image = detect_obstacles(robot_pose, unknown_obstacles, target_pose_dict, map_image_copy)
    if detected:
        map_image_copy = updated_map_image

    #robot_pose = get_robot_pose(ekf,robot,lv,rv,dt)
    lv,rv,dt,robot_pose = drive_to_point(waypoint,robot_pose)
    unknown_obstacles, detected, detected_location, updated_map_image = detect_obstacles(robot_pose, unknown_obstacles, target_pose_dict, map_image_copy)
    if detected:
        map_image_copy = updated_map_image
    #robot_pose = get_robot_pose(ekf,robot,lv,rv,dt)
    print(f"Clicked coordinates: ({x:.2f}, {y:.2f})")
    
    # Update the robot pose
    #this was just for testing GUI need to use control algorithm to update the robot pose
    # call drive_to_point function here and use x,y as inputs, along with current robot pose
    #theta = math.atan2(robot_pose[1]-y,  robot_pose[0]-x)
    #robot_pose = [x, y, theta] 
    print(f"Updated robot pose: [{robot_pose[0]}, {robot_pose[1]}, {(180/math.pi)*robot_pose[2]}]")
    
    # Redraw the map image to clear previous circles
    map_image_copy = map_image.copy()
    draw_robot(event.x, event.y, robot_pose[2])

#drawing the 0.5m radius circles around the target fruits, that the robot needs to be within to pick them up
def draw_target_circles(image, targets):
    draw = ImageDraw.Draw(image)
    img_width, img_height = image.size
    target_numbers = [1, 2, 3, 4, 5]
    counter = 0
    
    for target in targets:
        # Convert target coordinates to pixel coordinates
        # based on the direction of your x/y axis my need to adjust x and y (+/-) to get the correct orientation
        x = int((target[0] + 1.5) / 3 * img_width)
        y = int((1.5 - target[1]) / 3 * img_height)
        
        # Draw a circle with a 0.5m radius around the target
        radius = int(0.5 / 3 * img_width)  # Convert 0.5m to pixel radius
        draw.ellipse((x - radius, y - radius, x + radius, y + radius), outline='blue', width=1)  # Increase width for better visibility
        #numbering the targets on the map so viewer can see the order them must go in
        #make sure orientation arrow is pointing to the target in order to "collect" it
        draw.text((x, y), str(target_numbers[counter]), fill='white', anchor='mm')
        counter += 1

# Draw the robot on the map image to show its current position and orientation
def draw_robot(x,y,orientation):
    global map_image_copy
    # Draw a red circle on the image at the specified coordinates
    draw = ImageDraw.Draw(map_image_copy)
    radius = 10
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill='red')

    # Draw an arrow to represent the robot's orientation
    orientation = robot_pose[2] +math.pi
    # Calculate the end point of the arrow
    arrow_length = 20  # Length of the arrow
    end_x = x + arrow_length * math.cos(orientation)
    end_y = y - arrow_length * math.sin(orientation)  # Invert y-axis for drawing

    # Draw the arrow
    draw.line((x, y, end_x, end_y), fill='red', width=3)

    
    # Update the image in the label
    map_photo.paste(map_image_copy)

def process_waypoints(x, y):
    # Convert pixel coordinates to actual waypoints if needed
    # For now, just print them
    print(f"Waypoint coordinates: ({x}, {y})")
    # estimate the robot's pose
    robot_pose = get_robot_pose()
    print(f"Robot pose: {robot_pose}")

# creates the map image based on the map given in the .txt file
# helpful for visualizing the robot's position and the target fruits, and placing waypoints
# allows for quick testing on different maps
def create_map_image(fruits_true_pos, aruco_true_pos, width=500, height=500):
    # Create a blank image
    map_image = Image.new('RGB', (width, height), 'white')
    draw = ImageDraw.Draw(map_image)
    aruco_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9,10]
    
    # based on the direction of your x/y axis my need to adjust x and y (+/-) to get the correct orientation
    # Draw the fruits
    for pos in fruits_true_pos:
        x = int((1.5 - pos[0]) / 3 * width)
        y = int((pos[1] + 1.5) / 3 * height)

        radius = 10  # Radius of the fruit
        draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill='green')
    
    # Draw the ArUco markers
    for i,pos in enumerate(aruco_true_pos):
        x = int((1.5 - pos[0]) / 3 * width)
        y = int((pos[1] + 1.5) / 3 * height)
        side = 20  # Side length of the square
        #drawing black blocks for markers
        draw.rectangle((x - side // 2, y - side // 2, x + side // 2, y + side // 2), fill='black')
        #numbering the blocks
        draw.text((x, y), str(aruco_numbers[i]), fill='white', anchor='mm')

    # Draw the robot at the initial position
    robot_x = int((1.5 - robot_pose[0]) / 3 * width)
    robot_y = int((robot_pose[1] + 1.5) / 3 * height)
    radius = 5  # Radius of the robot
    draw.ellipse((robot_x - radius, robot_y - radius, robot_x + radius, robot_y + radius), fill='red')
    
    return map_image

# main loop
if __name__ == "__main__":
    parser = argparse.ArgumentParser("Fruit searching")
    parser.add_argument("--map", type=str, default='M4_prac_map_part.txt') # change to 'M4_true_map_part.txt' for lv2&3
    parser.add_argument("--ip", metavar='', type=str, default='192.168.50.1')
    parser.add_argument("--port", metavar='', type=int, default=8080)
    args, _ = parser.parse_known_args()

    ppi = PenguinPi(args.ip,args.port)

    # read in the true map
    fruits_list, fruits_true_pos, aruco_true_pos = read_true_map(args.map)
    search_list = read_search_list()
    targetPose = print_target_fruits_pos(search_list, fruits_list, fruits_true_pos)

    combined_positions = np.vstack((fruits_true_pos, aruco_true_pos))



    fileS = "calibration/param/scale.txt"
    scale = np.loadtxt(fileS, delimiter=',')
    fileB = "calibration/param/baseline.txt"
    baseline = np.loadtxt(fileB, delimiter=',')
    fileK = "calibration/param/intrinsic.txt"
    camera_matrix = np.loadtxt(fileK, delimiter=',')
    fileD = "calibration/param/distCoeffs.txt"
    dist_coeffs = np.loadtxt(fileD, delimiter=',')

    script_dir = os.path.dirname(os.path.abspath(__file__))
    robot = Robot(baseline, scale, camera_matrix, dist_coeffs)
    ekf = EKF(robot)
    model_path = f'{script_dir}/YOLO/model/yolov8_model.pt'
    yolo = Detector(model_path)
    for i in range(len(aruco_true_pos)):
        x , y = aruco_true_pos[i]
        ekf.add_true_landmarks(i+1,x,y)

    waypoint = [0.0,0.0]
    robot_pose = [0.0,0.0,0.0]
    target_pose_dict = {}
    unknown_obstacles = []

    # Create the main window
    root = tk.Tk()
    root.title("Map Click Waypoints")

    # Load the map image
    #map_image = Image.open("M4_prac_map_layout_cropped.png")  # Update with your map image path
    map_image = create_map_image(fruits_true_pos, aruco_true_pos)

    #map_image_copy = map_image.copy()
    draw_target_circles(map_image, targetPose)
    map_photo = ImageTk.PhotoImage(map_image)

    # Create a label to display the map image
    map_label = tk.Label(root, image=map_photo)
    map_label.pack()

    # Bind the click event to the on_click function
    map_label.bind("<Button-1>", on_click)

    # Bind the spacebar event to the on_space function
    root.bind("<KeyPress-space>", on_space)

    # Start the GUI event loop
    root.mainloop()

    print(f"Updated robot pose: [{robot_pose[0]}, {robot_pose[1]}, {(180/math.pi)*robot_pose[2]}]")

        # robot drives to the waypoint
       # waypoint = [x,y]
        #drive_to_point(waypoint,robot_pose)
        #print("Finished driving to waypoint: {}; New robot pose: {}".format(waypoint,robot_pose))

        # exit
        #ppi.set_velocity([0, 0])
        #uInput = input("Add a new waypoint? [Y/N]")
        #if uInput == 'N':
        #    break

